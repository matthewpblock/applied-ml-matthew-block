# Project 02

## Overview

Classification is a fundamental task in machine learning where the goal is to predict a categorical label. For example, a company might want to predict whether a customer will churn, or a hospital might want to classify a tumor as benign or malignant. Classification models learn from labeled data to identify patterns that distinguish between different classes.

This project demonstrates the application of classification modeling to a historical dataset. You will:

- Load and explore a dataset suitable for classification.
- Choose and justify features for predicting a target class.
- Prepare the data through cleaning and feature engineering.
- Train a classification model and evaluate its performance.
- Document your work in a structured Jupyter Notebook.

## Dataset

**Titanic Survival Dataset** (Predict passenger survival)

- We use the built-in dataset from the Seaborn library:
  - `import seaborn as sns`
  - `titanic = sns.load_dataset('titanic')`
- This classic dataset contains demographic and travel information for passengers aboard the RMS Titanic, with the goal of predicting whether a passenger survived the sinking.

## Python Library for Machine Learning: scikit-learn

We use scikit-learn, built on NumPy, SciPy, and matplotlib.
- Read more at <https://scikit-learn.org/>
- Scikit-learn supports classification, regression, and clustering.
- This project applies classification.

## Professional Python Setup and Workflow

We follow professional Python practices.
Full instructions are available in the main README.md file for this repository.

---

## Project Outline

Machine learning projects follow a structured approach. This notebook is organized into the following sections:

### Section 1. Load and Explore the Data

- 1.1 Load the dataset and display the first 10 rows.
- 1.2 Check for missing values and display summary statistics.
- **Analysis**: Initial observations about the dataset, data types, and potential data quality issues.

### Section 2. Exploration & Preparation

- 2.1 Visualize data patterns and distributions (histograms, scatterplots).
- 2.2 Handle missing values through imputation.
- 2.3 Engineer new features like `family_size` and convert categorical data to numeric.
- **Analysis**: Insights gained from visualizations and justification for data preparation choices.

### Section 3. Feature Selection and Justification

- 3.1 Choose input features (`pclass`, `fare`, `adult_male`, `alone`) for predicting the target.
- 3.2 Select the target variable (`survived`).
- **Analysis**: Justification for feature selection based on correlation and domain knowledge.

### Section 4. Split Testing

- 4.1 Split the data into training and test sets using `train_test_split` and `StratifiedShuffleSplit`.
- 4.2 Compare the class distributions of the splits.
- **Analysis**: Discussion on why stratification is important for maintaining class balance and improving model performance.

# Credits and Acknowledgements
- This project was completed as part of the Applied Machine Learning course at Northwest Missouri State University under the instrucion of Dr. Denise Case.  
- The majority of this README was generated by Gemini Code Assist in VS Code.